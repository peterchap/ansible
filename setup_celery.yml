---
- name: Set up Celery on slave servers
  hosts: badslaves
  become: yes

  vars:
    project_dir: /root/celery_app
    venv_dir: "{{ project_dir }}/venv"  # Virtual environment directory
    master_rabbitmq_ip: "10.0.0.9"  # Replace with your master server's IP
    master_redis_ip: "10.0.0.9"     # Replace with your master server's IP
    input_folder: "/mnt/shared/subfiles"  # Path to the shared NFS folder
    processed_folder: "/mnt/shared/processed"  # Path to the processed folder in NFS

  tasks:
    - name: Delete content & directory
      ansible.builtin.file:
        state: absent
        path: /root/celery_app/

    - name: Install Python3 and pip3
      apt:
        name:
          - python3.12
          - python3-pip
          - python3-venv
        state: present
        update_cache: yes

    - name: Create project directory
      file:
        path: "{{ project_dir }}"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Create virtual environment
      command: python3.12 -m venv {{ venv_dir }}


    - name: Install Celery and Redis client for Python
      command: "{{ venv_dir }}/bin/pip install polars celery[redis]"

    - name: Create celery_app.py file in project directory
      copy:
        dest: "{{ project_dir }}/celery_app.py"
        content: |
          from kombu import Queue
          from celery import Celery

          app = Celery('task',
                      broker='pyamqp://rabbit:1Francis2@{{ master_rabbitmq_ip }}//')

          app.conf.task_queues = [
              Queue('file_queue')
          ]

          app.conf.broker_connection_retry_on_startup = True

          # Import tasks to register them with Celery
          import task

        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'

    - name: Create task.py file in project directory
      copy:
        dest: "{{ project_dir }}/task.py"
        content: |
          import logging
          from celery_app import app  # Import the Celery app from celery_app.py

          # Configure logging
          logging.basicConfig(
              filename='process.log',
              level=logging.INFO,
              format='%(asctime)s %(levelname)s:%(message)s'
          )

          @app.task(name="task.process_file")
          def process_file(filename):
              #Task to process a Parquet file, count records, and write the result.
              input_folder = "/mnt/shared/subfiles/"
              processed_folder = "/mnt/shared/processed/"
              file_path = input_folder + filename
              processed_file_path = processed_folder + "processed" + filename
              # Read the Parquet file

              logging.info(f"Started processing file: {filename}")
              try:
                 data = pl.scan_parquet(file_path).select(pl.count()).collect()
                 data.write_parquet(processed_file_path)
                 logging.info(f"Processed file at path: {file_path}")
              except Exception as e:
                 logging.error(f"Failed to read {file_path}: {str(e)}")
                 raise

        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'


    - name: Create Celery systemd service file for worker
      copy:
        dest: /etc/systemd/system/celery_worker.service
        content: |
          [Unit]
          Description=Celery Worker
          After=network.target

          [Service]
          User=root
          Group=root
          WorkingDirectory=/root/celery_app/
          ExecStart=/bin/bash -c 'source /root/celeryapp/bin/activate && /root/celeryapp/bin/celery -A celery_app worker --loglevel=info -Q file_queue'

          # Correct redirection of logs
          StandardOutput=file:/root/celery_app/celery_worker.log
          StandardError=file:/root/celery_app/celery_worker_error.log

          Restart=always
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
        mode: '0644'

    - name: Reload systemd to apply the new service
      systemd:
        daemon_reload: yes

    - name: Enable and start the Celery worker service
      systemd:
        name: celery_worker
        enabled: yes
        state: started
